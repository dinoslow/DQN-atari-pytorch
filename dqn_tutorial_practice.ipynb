{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"dqn_tutorial_practice.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOjmKVQWLu40L4EcnfAQYHQ"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"cl30FhZS-Ir8"},"source":["# Deep Q Network Tutorial"]},{"cell_type":"markdown","metadata":{"id":"NuUUk1ttKXAQ"},"source":["## 1. Environment Preparation\n","### 1.1 Mount drive and set project path."]},{"cell_type":"code","metadata":{"id":"jK0QCNk9v4W8"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import sys\n","project_root = '/content/drive/My Drive/DQN_tutorial/'\n","sys.path.append(project_root)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7HN4h_eeKhq6"},"source":["### 1.2 Download Atari ROM."]},{"cell_type":"code","metadata":{"id":"2J_47p11hM7y"},"source":["! wget http://www.atarimania.com/roms/Roms.rar\n","! mkdir /content/ROM/\n","! unrar e /content/Roms.rar /content/ROM/\n","! python -m atari_py.import_roms /content/ROM/"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QHo6IpBN92O7"},"source":["## 2. Pong Game and Wrapper\n","### 2.1 Test the pong environment."]},{"cell_type":"code","metadata":{"id":"g_SMe9nq91OI"},"source":["import gym\n","import matplotlib.pyplot as plt\n","\n","env_name = \"PongNoFrameskip-v4\"\n","env = gym.make(env_name)\n","\n","print(\"environment:\", env_name)\n","print(\"action space:\", env.action_space.n)\n","print(\"action:\", env.unwrapped.get_action_meanings())\n","print(\"observation space:\", env.observation_space.shape)\n","\n","state = env.reset()\n","action = env.action_space.sample()\n","state_next, reward, done, info = env.step(action)\n","plt.figure()\n","plt.imshow(state_next)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y3S0ZQ_4AamT"},"source":["### 2.2 Environment wrapper."]},{"cell_type":"code","metadata":{"id":"6N8dWp49AUZC"},"source":["import numpy as np\n","from PIL import Image\n","\n","class PongEnvWrapper(gym.Wrapper):\n","    def __init__(self, env, k, img_size=(84,84)):\n","        gym.Wrapper.__init__(self, env)\n","        self.k = k\n","        self.img_size = img_size\n","        obs_shape = env.observation_space.shape\n","        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(k, img_size[0], img_size[1]), dtype=np.float32)\n","\n","    def _preprocess(self, state, th=0.4):\n","        # TODO(Lab-1): Image processing.\n","        return state\n","\n","    def reset(self):\n","        state = self.env.reset()\n","        state = self._preprocess(state)\n","        # TODO(Lab-2): Constrct initial stacked frame.\n","        return state\n","\n","    def step(self, action):\n","        # TODO(Lab-3): Construct stacked frames.\n","        return state_next, reward, done, info"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HPIdul6SAqg4"},"source":["# Test Code\n","env_pong = PongEnvWrapper(env, k=4, img_size=(84,84))\n","print(\"observation space:\", env_pong.observation_space.shape)\n","\n","state = env_pong.reset()\n","action = env_pong.action_space.sample()\n","state_next, reward, done, info = env_pong.step(action)\n","print(state_next.shape)\n","plt.imshow(state_next[0], cmap=\"gray\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C8PmVn3yCeOK"},"source":["## 3. Reinforcement Learning\n","### 3.1 Convolutional Neural Network"]},{"cell_type":"code","metadata":{"id":"sHItigfeFp1p"},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","class QNet(nn.Module):\n","    # TODO(Lab-4): Q-Network architecture."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I6FmitVZFyOk"},"source":["### 3.2 DQN Algorithm"]},{"cell_type":"code","metadata":{"id":"mZpRBeuXCol6"},"source":["class DeepQNetwork():\n","    def __init__(\n","        self,\n","        n_actions,\n","        input_shape,\n","        qnet,\n","        device,\n","        learning_rate = 2e-4,\n","        reward_decay = 0.99,\n","        replace_target_iter = 1000,\n","        memory_size = 10000,\n","        batch_size = 32,\n","    ):\n","        # initialize parameters\n","        self.n_actions = n_actions\n","        self.input_shape = input_shape\n","        self.lr = learning_rate\n","        self.gamma = reward_decay\n","        self.replace_target_iter = replace_target_iter\n","        self.memory_size = memory_size\n","        self.batch_size = batch_size\n","        self.device = device\n","        self.learn_step_counter = 0\n","        self.init_memory()\n","\n","        # Network\n","        self.qnet_eval = qnet(self.input_shape, self.n_actions).to(self.device)\n","        self.qnet_target = qnet(self.input_shape, self.n_actions).to(self.device)\n","        self.qnet_target.eval()\n","        self.optimizer = optim.RMSprop(self.qnet_eval.parameters(), lr=self.lr)\n","\n","    def choose_action(self, state, epsilon=0):\n","        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n","        actions_value = self.qnet_eval.forward(state)\n","        if np.random.uniform() > epsilon:   # greedy\n","            action = torch.max(actions_value, 1)[1].data.cpu().numpy()[0]\n","        else:   # random\n","            action = np.random.randint(0, self.n_actions)\n","        return action\n","\n","    def learn(self):\n","        # TODO(Lab-5): DQN core algorithm.\n","\n","    def init_memory(self):\n","        self.memory = {\n","            \"s\": np.zeros((self.memory_size, *self.input_shape)),\n","            \"a\": np.zeros((self.memory_size, 1)),\n","            \"r\": np.zeros((self.memory_size, 1)),\n","            \"s_\": np.zeros((self.memory_size, *self.input_shape)),\n","            \"done\": np.zeros((self.memory_size, 1)),\n","        }\n","\n","    def store_transition(self, s, a, r, s_, d):\n","        if not hasattr(self, 'memory_counter'):\n","            self.memory_counter = 0\n","        if self.memory_counter <= self.memory_size:\n","            index = self.memory_counter % self.memory_size\n","        else:\n","            index = np.random.randint(self.memory_size)\n","        self.memory[\"s\"][index] = s\n","        self.memory[\"a\"][index] = np.array(a).reshape(-1,1)\n","        self.memory[\"r\"][index] = np.array(r).reshape(-1,1)\n","        self.memory[\"s_\"][index] = s_\n","        self.memory[\"done\"][index] = np.array(d).reshape(-1,1)\n","        self.memory_counter += 1\n","    \n","    def save_load_model(self, op, path=\"save\", fname=\"qnet.pt\"):\n","        import os\n","        if not os.path.exists(path):\n","            os.makedirs(path)\n","        file_path = os.path.join(path, fname)\n","        if op == \"save\":\n","            torch.save(self.qnet_eval.state_dict(), file_path)\n","        elif op == \"load\":\n","            self.qnet_eval.load_state_dict(torch.load(file_path, map_location=self.device))\n","            self.qnet_target.load_state_dict(torch.load(file_path, map_location=self.device))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZeHuwGZSFa7c"},"source":["stack_frames = 4\n","img_size = (84,84)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","agent = DeepQNetwork(\n","        n_actions = env.action_space.n,\n","        input_shape = [stack_frames, *img_size],\n","        qnet = QNet,\n","        device = device,\n","        learning_rate = 2e-4, \n","        reward_decay = 0.99,\n","        replace_target_iter = 1000, \n","        memory_size = 10000,\n","        batch_size = 32,)\n","\n","print(agent.qnet_eval)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TlO1ozgFKsjC"},"source":["## 4. Training and testing process.\n","### 4.1 Play the game."]},{"cell_type":"code","metadata":{"id":"6gC66Kh7LS87"},"source":["def play(env, agent, stack_frames, img_size):\n","    # Reset environment.\n","    state = env.reset()\n","    img_buffer = [Image.fromarray(state[0]*255)]\n","\n","    # Initialize information.\n","    step = 0\n","    total_reward = 0\n","\n","    # One episode.\n","    while True:\n","        # Select action.\n","        action = agent.choose_action(state, 0)\n","\n","        # Get next stacked state.\n","        state_next, reward, done, info = env.step(action)\n","        if step % 2 == 0:\n","            img_buffer.append(Image.fromarray(state_next[0]*255))\n","\n","        state = state_next.copy()\n","        step += 1\n","        total_reward += reward\n","        print('\\rStep: {:3d} | Reward: {:.3f} / {:.3f}'\\\n","            .format(step, reward, total_reward), end=\"\")\n","            \n","        if done or step>2000:\n","            print()\n","            break\n","\n","    return img_buffer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4h71uAIcFVpj"},"source":["import os\n","def save_gif(img_buffer, fname, gif_path=os.path.join(project_root, \"gif\")):\n","    if not os.path.exists(gif_path):\n","        os.makedirs(gif_path)\n","    img_buffer[0].save(os.path.join(gif_path, fname), save_all=True, append_images=img_buffer[1:], duration=1, loop=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IwvsjrHMTyOh"},"source":["# Test Code\n","img_buffer = play(env_pong, agent, stack_frames, img_size)\n","save_gif(img_buffer, fname=\"test.gif\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zaeYbrDBScJq"},"source":["### 4.2 Epsilon greedy computation."]},{"cell_type":"code","metadata":{"id":"Z4CLYqqOSiHH"},"source":["def epsilon_compute(frame_id, epsilon_max=1, epsilon_min=0.05, epsilon_decay=100000):\n","    return epsilon_min + (epsilon_max - epsilon_min) * np.exp(-frame_id / epsilon_decay)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tkS_5piBUU98"},"source":["# Test Code\n","frame_ids = np.array(range(400000))\n","epsilons = epsilon_compute(frame_ids)\n","plt.plot(epsilons)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XrD-BxWCK-pa"},"source":["### 4.3 Training steps."]},{"cell_type":"code","metadata":{"id":"hj3uuC2w0JEr"},"source":["def train(env, agent, stack_frames, img_size, save_path=\"save\", max_steps=1000000):\n","    total_step = 0\n","    episode = 0\n","    while True:\n","        # Reset environment.\n","        state = env.reset()\n","\n","        # Initialize information.\n","        step = 0\n","        total_reward = 0\n","        loss = 0\n","\n","        # One episode.\n","        while True:\n","            # TODO(Lab-6): Select action.\n","\n","            # Get next stacked state.\n","            state_next, reward, done, info = env.step(action)\n","\n","            # TODO(Lab-7): Train RL model.\n","\n","            state = state_next.copy()\n","            step += 1\n","            total_step += 1\n","            total_reward += reward\n","\n","            if total_step % 100 == 0 or done:\n","                print('\\rEpisode: {:3d} | Step: {:3d} / {:3d} | Reward: {:.3f} / {:.3f} | Loss: {:.3f} | Epsilon: {:.3f}'\\\n","                    .format(episode, step, total_step, reward, total_reward, loss, epsilon), end=\"\")\n","            \n","            if total_step % 10000 == 0:\n","                print(\"\\nSave Model ...\")\n","                agent.save_load_model(op=\"save\", path=save_path, fname=\"qnet.pt\")\n","                print(\"Generate GIF ...\")\n","                img_buffer = play(env, agent, stack_frames, img_size)\n","                save_gif(img_buffer, \"train_\" + str(total_step).zfill(6) + \".gif\")\n","                print(\"Done !!\")\n","\n","            if done or step>2000:\n","                episode += 1\n","                print()\n","                break\n","        \n","        if total_step > max_steps:\n","            break"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mHki_ukzyEpF"},"source":["train(env_pong, agent, stack_frames, img_size, save_path=os.path.join(project_root, \"save\"), max_steps=400000)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Igf0N-DQLC_u"},"source":["### 4.4 Evaluate the trained model."]},{"cell_type":"code","metadata":{"id":"KTVutI83Iv4H"},"source":["agent.save_load_model(op=\"load\", path=os.path.join(project_root, \"save\"), fname=\"qnet.pt\")\n","img_buffer = play(env_pong, agent, stack_frames, img_size)\n","save_gif(img_buffer, \"eval.gif\")"],"execution_count":null,"outputs":[]}]}